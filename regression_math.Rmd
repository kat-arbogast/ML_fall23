---
title: "Regression Math Overview Stuff"
author: "Katrina Arbogast"
output: html_document
---


# Regression Stuff


\begin{align}
\quad \\
\quad \\
\quad \\
\quad \\
\end{align}


\begin{align}
\begin{bmatrix}
    Y_{1}  \\
    Y_{2}  \\
    \vdots  \\
    Y_{n}  \\
\end{bmatrix} = 

\begin{bmatrix}
    1 & X_{11} & X_{12} & \cdots & X_{1p} \\
    1 & X_{21} & X_{22} & \cdots & X_{2p} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & X_{n1} & X_{n2} & \cdots & X_{np} \\
\end{bmatrix}

\begin{bmatrix}
    \beta_{0}  \\
    \beta_{1}  \\
    \vdots  \\
    \beta_{p}  \\
\end{bmatrix} +

\begin{bmatrix}
    \varepsilon_{1}  \\
    \varepsilon_{2}  \\
    \vdots  \\
    \varepsilon_{n}  \\
\end{bmatrix}

\end{align}




\begin{align}
\quad \\
\quad \\
\quad \\
\quad \\
\end{align}

### Gauss-Markov Assumptions

In a linear regression with response vector $\mathbf{Y}$ and design matrix $\mathbf{X}$, the least squares estimator $\widehat{\boldsymbol\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}$ is the minimum-variance linear unbiased estimator of the model parameter, under the ordinary least squares assumptions. Where these assumptions are as follows:

\begin{align}
\begin{array}{@{}l@{\quad}l@{}}
(1) & E[\varepsilon_i] = 0 \\
(2) & E[Y_i] = X_i^T\boldsymbol\beta \quad \forall i=1,...,n \\
(3) & \text{Cov}(\varepsilon_i, \varepsilon_j) = \begin{cases} 0 & i \neq j \\ \sigma^2  & i=j \end{cases} \\
(4) & (\mathbf{X}^T\mathbf{X})^{-1} \quad \text{exists}
\end{array}
\end{align}


\begin{align}
\quad \\
\quad \\
\quad \\
\quad \\
\end{align}



```{r}
# Install and load the ggplot2 package if not already installed
# install.packages("ggplot2")
library(ggplot2)

# Sample data creation
set.seed(123)  # Set seed for reproducibility
n <- 30
x <- rnorm(n)
y <- 2 * x + rnorm(n)

# Create a data frame
data <- data.frame(x, y)

# Calculate the average y
average_y <- mean(y)

# Fit a linear regression model
lm_model <- lm(y ~ x, data = data)

# Calculate the differences
diff_y_bar <- y - average_y
diff_y_fit <- residuals(lm_model)
diff_fit_bar <- fitted(lm_model) - average_y

# Plotting
ggplot(data, aes(x, y)) +
  theme_minimal() +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  geom_hline(yintercept = average_y, color = "darkgrey", label = expression(bar(y))) +
  #geom_segment(aes(x = x[1:3], y = y[1:3], xend = x[1:3], yend = average_y), color = "green", linetype = "dashed") +
  #geom_segment(aes(x = x[1:3], y = y[1:3], xend = x[1:3], yend = fitted(lm_model)[1:3]), color = "purple", linetype = "dashed") +
  #geom_segment(aes(x = x[1:3], y = fitted(lm_model)[1:3], xend = x[1:3], yend = average_y), color = "orange", linetype = "dashed") +
  #annotate("text", x = x[1:3], y = y[1:3] - 0.2, label = sprintf("Diff_y_bar = %.2f", diff_y_bar[1:3]), color = "green") +
  #annotate("text", x = x[1:3], y = (y[1:3] + fitted(lm_model)[1:3]) / 2, label = sprintf("Diff_y_fit = %.2f", diff_y_fit[1:3]), color = "purple") +
  #annotate("text", x = x[1:3], y = fitted(lm_model)[1:3] + 0.2, label = sprintf("Diff_fit_bar = %.2f", diff_fit_bar[1:3]), color = "orange") +
  labs(title = "",
       x = "",
       y = "")

```

